{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:09:59.661306Z",
     "start_time": "2019-08-26T02:09:58.404545Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Iterator\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as ptl\n",
    "from test_tube import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from millenlp.embeddings import FastTextVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder with pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:09:59.812320Z",
     "start_time": "2019-08-26T02:09:59.800137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:10:00.834427Z",
     "start_time": "2019-08-26T02:10:00.828463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:10:01.026418Z",
     "start_time": "2019-08-26T02:10:01.021414Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:10:01.179289Z",
     "start_time": "2019-08-26T02:10:01.172715Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T22:53:11.551238Z",
     "start_time": "2019-08-22T22:53:08.134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.30)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.5)\n",
    "data_train.shape, data_val.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T22:53:11.552062Z",
     "start_time": "2019-08-22T22:53:08.667Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.to_csv('dataset/data_train.csv')\n",
    "data_val.to_csv('dataset/data_val.csv')\n",
    "data_test.to_csv('dataset/data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:10:12.398074Z",
     "start_time": "2019-08-26T02:10:02.856772Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_es = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:10:12.401656Z",
     "start_time": "2019-08-26T02:10:12.399197Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes Spanish text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_es.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:36:34.413701Z",
     "start_time": "2019-08-26T02:36:34.395524Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(fix_length=100, lower=False, vectors=None):\n",
    "    if vectors is not None:\n",
    "        # pretrain vectors only supports all lower cases\n",
    "        lower = True\n",
    "        \n",
    "    comment = Field(\n",
    "        sequential=True,\n",
    "        fix_length=fix_length,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "        batch_first = True,\n",
    "        lower=lower\n",
    "    )\n",
    "    train, val = TabularDataset.splits(\n",
    "        path='dataset/', format='csv', skip_header=True,\n",
    "        train='data_train.csv', validation='data_val.csv',\n",
    "        fields=[\n",
    "            ('mensaje', None),\n",
    "            ('lemma', comment),\n",
    "            ('cluster', None),\n",
    "            ('cluster_2', None),\n",
    "            ('output', comment)\n",
    "        ])\n",
    "    test = TabularDataset(\n",
    "        path='dataset/data_test.csv', format='csv', \n",
    "        skip_header=True,\n",
    "        fields=[\n",
    "            ('mensaje', None),\n",
    "            ('lemma', comment),\n",
    "            ('cluster', None),\n",
    "            ('cluster_2', None)\n",
    "        ])\n",
    "\n",
    "    comment.build_vocab(\n",
    "        train, val, test,\n",
    "        max_size=20000,\n",
    "        min_freq=50,\n",
    "        vectors=vectors\n",
    "    )\n",
    "    return train, val, test, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T02:36:59.679125Z",
     "start_time": "2019-08-26T02:36:36.024948Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "train_dataset, val_dataset, test_dataset, message_field = get_dataset(fix_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T03:17:39.205064Z",
     "start_time": "2019-08-26T03:17:39.193954Z"
    },
    "code_folding": [
     1,
     37,
     44,
     46,
     52
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size, \n",
    "                 message_field, \n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.bidir = bidir\n",
    "        self.message_field = message_field\n",
    "        self.embedding_dim = input_size\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(num_embeddings=len(self.message_field.vocab.itos),\n",
    "                                           embedding_dim=self.embedding_dim,\n",
    "                                           padding_idx=self.message_field.vocab.stoi['<pad>']).to(DEVICE)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                                  hidden_size=self.hidden_size,\n",
    "                                  num_layers=self.layers,\n",
    "                                  batch_first=True,\n",
    "                                  dropout = self.dropout if self.dropout and self.layers > 1 else 0,\n",
    "                                  bidirectional = self.bidir).to(DEVICE)\n",
    "        \n",
    "#         self.dense1 = torch.nn.Linear(self.units*2 if self.bidir else self.units, self.dim_reduction[0]).to(DEVICE)\n",
    "#         self.bn1 = torch.nn.BatchNorm1d(num_features=self.dim_reduction[0]).to(DEVICE)\n",
    "#         self.dense2 = torch.nn.Linear(self.dim_reduction[0]*2 if self.bidir else self.dim_reduction[0], \n",
    "#                                      self.dim_reduction[1]).to(DEVICE)\n",
    "        \n",
    "        self.init_weigths()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden = torch.randn(self.layers*2 if self.bidir else self.layers, self.batch_size, self.hidden_size).to(DEVICE)\n",
    "        cell = torch.randn(self.layers*2 if self.bidir else self.layers, self.batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "        return (hidden, cell)\n",
    "    \n",
    "    def init_weigths(self):\n",
    "        \n",
    "        for param in self.lstm.named_parameters():\n",
    "            if 'weight' in param[0]:\n",
    "                torch.nn.init.orthogonal_(param[1])\n",
    "#         torch.nn.init.xavier_normal_(self.dense.weight)\n",
    "        print('weigths initializer: done!')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.word_embedding(x)\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden()\n",
    "        x, (self.hidden, self.cell) = self.lstm(x, (self.hidden, self.cell))\n",
    "        \n",
    "        last_hidden = self.hidden.view(self.layers,2,-1,self.hidden_size)[-1] if self.bidir else self.hidden[-1]\n",
    "        last_hidden = last_hidden.contiguous()\n",
    "        output = last_hidden.view(-1, self.hidden_size*2 if self.bidir else self.hidden_size)\n",
    "        \n",
    "#         output = self.dropout(F.relu(self.bn1(self.dense1(last_hidden))))\n",
    "#         output = self.dense2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T03:17:42.857167Z",
     "start_time": "2019-08-26T03:17:42.823634Z"
    },
    "code_folding": [
     1,
     36,
     43,
     51
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size,\n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.bidir = bidir\n",
    "        \n",
    "#         self.dense1 = torch.nn.Linear(self.dim_reduction[0]*2 if self.bidir else self.dim_reduction[0], \n",
    "#                                       self.dim_reduction[1]).to(DEVICE)\n",
    "#         self.bn1 = torch.nn.BatchNorm1d(num_features=self.dim_reduction[1]).to(DEVICE)\n",
    "        \n",
    "#         self.dense2 = torch.nn.Linear(self.dim_reduction[1]*2 if self.bidir else self.dim_reduction[1], \n",
    "#                                      self.units).to(DEVICE)\n",
    "#         self.bn2 = torch.nn.BatchNorm1d(num_features=self.units).to(DEVICE)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=self.layers,\n",
    "                            batch_first=True,\n",
    "                            dropout = self.dropout if self.dropout and self.layers > 1 else 0,\n",
    "                            bidirectional = self.bidir).to(DEVICE)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.init_weigths()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden = torch.randn(self.layers*2 if self.bidir else self.layers, self.batch_size, self.hidden_size).to(DEVICE)\n",
    "        cell = torch.randn(self.layers*2 if self.bidir else self.layers, self.batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "        return (hidden, cell)\n",
    "    \n",
    "    def init_weigths(self):\n",
    "        \n",
    "        for param in self.lstm.named_parameters():\n",
    "            if 'weight' in param[0]:\n",
    "                torch.nn.init.orthogonal_(param[1])\n",
    "#         torch.nn.init.xavier_normal_(self.dense.weight)\n",
    "        print('weigths initializer: done!')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         x = self.dropout(F.relu(self.bn1(self.dense1(x))))\n",
    "#         x = self.dropout(F.relu(self.bn2(self.dense2(x))))\n",
    "        \n",
    "        self.hidden, self.cell = self.init_hidden()\n",
    "        x, (self.hidden, self.cell) = self.lstm(x, (self.hidden, self.cell))\n",
    "        last_hidden = self.hidden.view(self.layers,2,-1,self.hidden_size)[-1] if self.bidir else self.hidden[-1]\n",
    "        last_hidden = last_hidden.contiguous()\n",
    "        last_hidden = last_hidden.view(-1,self.hidden_size*2 if self.bidir else self.hidden_size)\n",
    "        \n",
    "        output = self.softmax(last_hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T03:27:30.622155Z",
     "start_time": "2019-08-26T03:27:30.592814Z"
    },
    "code_folding": [
     39,
     62
    ]
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size, \n",
    "                 message_field, \n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256,\n",
    "                 sequence_length = 100):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = Encoder(layers = layers, \n",
    "                               hidden_size = self.hidden_size, \n",
    "                               input_size = input_size,\n",
    "                               message_field = message_field,\n",
    "                               dropout = dropout, \n",
    "                               bidir = bidir, \n",
    "                               batch_size = batch_size).to(DEVICE)\n",
    "        \n",
    "        self.decoder = Decoder(layers = layers, \n",
    "                               hidden_size = input_size, \n",
    "                               input_size = self.hidden_size,\n",
    "                               dropout = dropout, \n",
    "                               bidir = bidir, \n",
    "                               batch_size = batch_size).to(DEVICE)\n",
    "        \n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x).unsqueeze(1).expand(self.batch_size, self.sequence_length, self.hidden_size)\n",
    "        print(x.shape)\n",
    "        out = self.decoder(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def my_loss(self, y_hat, y):\n",
    "        return self.loss(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        (x, y), _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y = self.encoder.word_embedding(y)\n",
    "        return {'loss': self.my_loss(y_hat, y)}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        (x, y), _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        print(y_hat.shape)\n",
    "        y = self.encoder.word_embedding(y)\n",
    "        print(y.shape)\n",
    "        return {'val_loss': self.my_loss(y_hat, y)}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss, 'avg_val_acc':avg_val}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=5e-4, amsgrad = True)]\n",
    "    \n",
    "    @ptl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        return BucketIterator(train_dataset, batch_size=self.batch_size, device=DEVICE)\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return BucketIterator(val_dataset, batch_size=self.batch_size, device=DEVICE)\n",
    "    \n",
    "    @ptl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return BucketIterator(test_dataset, batch_size=self.batch_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T03:27:31.756857Z",
     "start_time": "2019-08-26T03:27:31.665559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weigths initializer: done!\n",
      "weigths initializer: done!\n",
      "VISIBLE GPUS: '0'\n",
      "gpu available: True, used: True\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(layers = 2,\n",
    "                    hidden_size = 2,\n",
    "                    input_size = 300,\n",
    "                    message_field = message_field,\n",
    "                    dropout = 0.2,\n",
    "                    bidir = False,\n",
    "                    batch_size=256,\n",
    "                    sequence_length = 100)\n",
    "\n",
    "exp = Experiment(save_dir=os.getcwd())\n",
    "trainer = ptl.Trainer(experiment=exp, max_nb_epochs=300, train_percent_check=1, gpus=[0],track_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T03:27:32.493159Z",
     "start_time": "2019-08-26T03:27:32.402191Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Name       Type   Params\n",
      "0                 encoder    Encoder   683480\n",
      "1  encoder.word_embedding  Embedding   681000\n",
      "2            encoder.lstm       LSTM     2480\n",
      "3                 decoder    Decoder  1087200\n",
      "4            decoder.lstm       LSTM  1087200\n",
      "5         decoder.softmax    Softmax        0\n",
      "6                    loss    MSELoss        0\n",
      "torch.Size([256, 100, 2])\n",
      "torch.Size([256, 300])\n",
      "torch.Size([256, 100, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-2b721ebf6dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# view tensorflow logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'and going to http://localhost:6006 on your browser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__single_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__single_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    788\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model, dataloader, max_batches, dataloader_i)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# RUN VALIDATION STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__validation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# track outputs for collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__validation_forward\u001b[0;34m(self, model, data_batch, batch_i, dataloader_i)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# do non dp, ddp step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-564f9b0b9aff>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-564f9b0b9aff>\u001b[0m in \u001b[0;36mmy_loss\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)\n",
    "\n",
    "# view tensorflow logs \n",
    "print(f'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}')\n",
    "print('and going to http://localhost:6006 on your browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

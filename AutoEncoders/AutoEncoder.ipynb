{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:28.201764Z",
     "start_time": "2019-09-06T16:35:26.328509Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "\n",
    "import spacy\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Iterator\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as ptl\n",
    "from test_tube import Experiment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from millenlp.embeddings import FastTextVec\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show, ColumnDataSource, output_file\n",
    "from bokeh.palettes import Blues9,Spectral11,Category10,Set1,Set2,Category20\n",
    "from bokeh.io import reset_output\n",
    "from bokeh.models import BoxSelectTool,CustomJS, ColumnDataSource, Legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:28.208149Z",
     "start_time": "2019-09-06T16:35:28.203167Z"
    }
   },
   "outputs": [],
   "source": [
    "output_file(\"Cluster.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:28.804831Z",
     "start_time": "2019-09-06T16:35:28.787539Z"
    },
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "def preping_bokeh_clustering(original_data,features,column,cluster):\n",
    "    \n",
    "    color_dic = {}\n",
    "    L = cluster['ClusterNumber'].unique().shape[0]\n",
    "    nums = [x for x in range(L)]\n",
    "    shuffle(nums)\n",
    "    if L <= 20:\n",
    "        for k,value in enumerate(cluster['ClusterNumber'].unique()):\n",
    "            if L >10:\n",
    "                color_dic[value] = Category20[20][nums[k]]\n",
    "            else:\n",
    "                color_dic[value] = Category10[10][nums[k]]\n",
    "    else:\n",
    "            \n",
    "        hexa = ['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f']\n",
    "        color_dic = {}\n",
    "        for k,value in enumerate(cluster['ClusterNumber'].unique()):\n",
    "            color_dic[value] = ''.join([choice(hexa) if i != 0 else '#' for i in range(7)])\n",
    "\n",
    "    df_bokeh = pd.DataFrame(np.concatenate((features,\n",
    "                                            original_data[column].values.reshape(-1,1),\n",
    "                                            original_data['lemma'].values.reshape(-1,1),\n",
    "                                            cluster['ClusterNumber'].values.reshape(-1,1)),axis=1),\n",
    "\n",
    "                            columns=['x','y','Message','lemma','Label'])\n",
    "    \n",
    "    df_bokeh['color'] = cluster.ClusterNumber.apply(lambda x: color_dic[x])\n",
    "    \n",
    "    TOOLTIPS = [(\"Index\", \"$index\"),\n",
    "            (\"(x,y)\", \"(@x, @y)\"),\n",
    "            (\"Message\", \"@{Message}\"),\n",
    "            (\"Lemmas\", \"@{lemma}\"),\n",
    "            (\"Label\", \"@{Label}\")]\n",
    "\n",
    "    return df_bokeh, TOOLTIPS\n",
    "\n",
    "def scatter(source,TOOLTIPS,classes):\n",
    "    p = figure(title=\"Fasttext and TSNE\", \n",
    "           x_axis_label='x', y_axis_label='y',\n",
    "           plot_width=950, plot_height=500, \n",
    "           tools = 'lasso_select,box_zoom,pan,poly_select,tap,wheel_zoom,save,zoom_out,crosshair,hover,reset,help',\n",
    "           tooltips=TOOLTIPS)\n",
    "    \n",
    "    legend_it = []\n",
    "    cluster_size = []\n",
    "    for label in range(0,classes):\n",
    "        cluster_size.append((label,source[source['Label']==label].shape[0]))\n",
    "    cluster_size = sorted(cluster_size, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for label in list(np.array(cluster_size)[:,0]):\n",
    "        c = p.scatter(x = \"x\",y = \"y\",size=5,\n",
    "                      line_color=\"black\", color=\"color\", \n",
    "                      alpha=0.7, source=ColumnDataSource(source[source['Label']==label]))\n",
    "        legend_it.append((str(label), [c]))\n",
    "    \n",
    "    legend = Legend(items=legend_it, location=(0, -30), spacing = 1)\n",
    "    legend.click_policy=\"hide\"\n",
    "    p.add_layout(legend, 'right')\n",
    "    \n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder with pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:29.877552Z",
     "start_time": "2019-09-06T16:35:29.871583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:22:23.587138Z",
     "start_time": "2019-09-04T16:22:23.583632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:22:23.782407Z",
     "start_time": "2019-09-04T16:22:23.780003Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:22:24.192607Z",
     "start_time": "2019-09-04T16:22:24.188286Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset/data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T22:53:11.551238Z",
     "start_time": "2019-08-22T22:53:08.134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.30)\n",
    "data_val, data_test = train_test_split(data_test, test_size=0.5)\n",
    "data_train.shape, data_val.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T22:53:11.552062Z",
     "start_time": "2019-08-22T22:53:08.667Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.to_csv('dataset/data_train.csv')\n",
    "data_val.to_csv('dataset/data_val.csv')\n",
    "data_test.to_csv('dataset/data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:46.349169Z",
     "start_time": "2019-09-06T16:35:32.887143Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_es = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:46.355767Z",
     "start_time": "2019-09-06T16:35:46.352101Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes Spanish text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_es.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:35:46.365967Z",
     "start_time": "2019-09-06T16:35:46.357739Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(fix_length=100, lower=False, vectors=None):\n",
    "    if vectors is not None:\n",
    "        # pretrain vectors only supports all lower cases\n",
    "        lower = True\n",
    "        \n",
    "    comment = Field(\n",
    "        sequential=True,\n",
    "        fix_length=fix_length,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "        batch_first = True,\n",
    "        lower=lower\n",
    "    )\n",
    "    train, val = TabularDataset.splits(\n",
    "        path='dataset/', format='csv', skip_header=True,\n",
    "        train='data_train.csv', validation='data_val.csv',\n",
    "        fields=[\n",
    "            ('mensaje', None),\n",
    "            ('lemma', comment),\n",
    "            ('cluster', None),\n",
    "            ('cluster_2', None),\n",
    "            ('output', comment)\n",
    "        ])\n",
    "    test = TabularDataset(\n",
    "        path='dataset/data_test.csv', format='csv', \n",
    "        skip_header=True,\n",
    "        fields=[\n",
    "            ('mensaje', None),\n",
    "            ('lemma', comment),\n",
    "            ('cluster', None),\n",
    "            ('cluster_2', None)\n",
    "        ])\n",
    "\n",
    "    comment.build_vocab(\n",
    "        train, val, test,\n",
    "        max_size=20000,\n",
    "        min_freq=50,\n",
    "        vectors=vectors\n",
    "    )\n",
    "    return train, val, test, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:36:14.392491Z",
     "start_time": "2019-09-06T16:35:46.368344Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "train_dataset, val_dataset, test_dataset, message_field = get_dataset(fix_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T22:27:39.083883Z",
     "start_time": "2019-09-06T22:27:39.068038Z"
    },
    "code_folding": [
     0,
     1,
     32,
     39,
     41,
     47
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size, \n",
    "                 message_field, \n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.bidir = bidir\n",
    "        self.message_field = message_field\n",
    "        self.embedding_dim = input_size\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(num_embeddings=len(self.message_field.vocab.itos),\n",
    "                                           embedding_dim=self.embedding_dim,\n",
    "                                           padding_idx=self.message_field.vocab.stoi['<pad>']).to(DEVICE)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                                  hidden_size=self.hidden_size,\n",
    "                                  num_layers=self.layers,\n",
    "                                  batch_first=True,\n",
    "                                  dropout = self.dropout if self.dropout and self.layers > 1 else 0,\n",
    "                                  bidirectional = self.bidir).to(DEVICE)\n",
    "        \n",
    "        self.init_weigths()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden = torch.randn(self.layers*2 if self.bidir else self.layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        cell = torch.randn(self.layers*2 if self.bidir else self.layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "        return (hidden, cell)\n",
    "    \n",
    "    def init_weigths(self):\n",
    "        \n",
    "        for param in self.lstm.named_parameters():\n",
    "            if 'weight' in param[0]:\n",
    "                torch.nn.init.orthogonal_(param[1])\n",
    "#         torch.nn.init.xavier_normal_(self.dense.weight)\n",
    "        print('weigths initializer: done!')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.word_embedding(x)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        x, (self.hidden, self.cell) = self.lstm(x, (self.hidden, self.cell))\n",
    "        \n",
    "        last_hidden = self.hidden.view(self.layers,2,-1,self.hidden_size)[-1] if self.bidir else self.hidden[-1]\n",
    "        last_hidden = last_hidden.contiguous()\n",
    "        output = last_hidden.view(-1, self.hidden_size*2 if self.bidir else self.hidden_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T16:36:14.416577Z",
     "start_time": "2019-09-06T16:36:14.406781Z"
    },
    "code_folding": [
     1,
     38,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size,\n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256,\n",
    "                 sequence_length = 100):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.bidir = bidir\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "#         self.dense1 = torch.nn.Linear(self.dim_reduction[0]*2 if self.bidir else self.dim_reduction[0], \n",
    "#                                       self.dim_reduction[1]).to(DEVICE)\n",
    "#         self.bn1 = torch.nn.BatchNorm1d(num_features=self.dim_reduction[1]).to(DEVICE)\n",
    "        \n",
    "#         self.dense2 = torch.nn.Linear(self.dim_reduction[1]*2 if self.bidir else self.dim_reduction[1], \n",
    "#                                      self.units).to(DEVICE)\n",
    "#         self.bn2 = torch.nn.BatchNorm1d(num_features=self.units).to(DEVICE)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=self.layers,\n",
    "                            batch_first=True,\n",
    "                            dropout = self.dropout if self.dropout and self.layers > 1 else 0,\n",
    "                            bidirectional = self.bidir).to(DEVICE)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.init_weigths()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden = torch.randn(self.layers*2 if self.bidir else self.layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "        cell = torch.randn(self.layers*2 if self.bidir else self.layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "        return (hidden, cell)\n",
    "    \n",
    "    def init_weigths(self):\n",
    "        \n",
    "        for param in self.lstm.named_parameters():\n",
    "            if 'weight' in param[0]:\n",
    "                torch.nn.init.orthogonal_(param[1])\n",
    "#         torch.nn.init.xavier_normal_(self.dense.weight)\n",
    "        print('weigths initializer: done!')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        self.hidden, self.cell = self.init_hidden(batch_size)\n",
    "        x, (self.hidden, self.cell) = self.lstm(x, (self.hidden, self.cell))\n",
    "        x = x.view(-1,self.sequence_length,2,self.hidden_size) if self.bidir else x\n",
    "        output = self.softmax(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T22:41:37.839862Z",
     "start_time": "2019-09-06T22:41:37.821825Z"
    },
    "code_folding": [
     1,
     34,
     41,
     50,
     56
    ]
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(ptl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 hidden_size, \n",
    "                 input_size, \n",
    "                 message_field, \n",
    "                 dropout = 0, \n",
    "                 bidir = False, \n",
    "                 batch_size = 256,\n",
    "                 sequence_length = 100):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = Encoder(layers = layers, \n",
    "                               hidden_size = self.hidden_size, \n",
    "                               input_size = input_size,\n",
    "                               message_field = message_field,\n",
    "                               dropout = dropout, \n",
    "                               bidir = bidir, \n",
    "                               batch_size = batch_size).to(DEVICE)\n",
    "        \n",
    "        self.decoder = Decoder(layers = layers, \n",
    "                               hidden_size = input_size, \n",
    "                               input_size = self.hidden_size,\n",
    "                               dropout = dropout, \n",
    "                               bidir = bidir, \n",
    "                               batch_size = batch_size,\n",
    "                               sequence_length = self.sequence_length).to(DEVICE)\n",
    "        \n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder(x).unsqueeze(1).expand(-1, self.sequence_length, self.hidden_size)\n",
    "        out = self.decoder(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def my_loss(self, y_hat, y):\n",
    "        return self.loss(y_hat, y)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        (x, y), _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y = self.encoder.word_embedding(y)\n",
    "        return {'loss': self.my_loss(y_hat, y)}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        (x, y), _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y = self.encoder.word_embedding(y)\n",
    "        return {'val_loss': self.my_loss(y_hat, y)}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return [torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=5e-4, amsgrad = True)]\n",
    "    \n",
    "    @ptl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        return BucketIterator(train_dataset, batch_size=self.batch_size, device=DEVICE)\n",
    "\n",
    "    @ptl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return BucketIterator(val_dataset, batch_size=self.batch_size, device=DEVICE)\n",
    "    \n",
    "    @ptl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return BucketIterator(test_dataset, batch_size=self.batch_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T22:41:39.485259Z",
     "start_time": "2019-09-06T22:41:39.378689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weigths initializer: done!\n",
      "weigths initializer: done!\n",
      "VISIBLE GPUS: '0'\n",
      "gpu available: True, used: True\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(layers = 2,\n",
    "                    hidden_size = 2,\n",
    "                    input_size = 300,\n",
    "                    message_field = message_field,\n",
    "                    dropout = 0.2,\n",
    "                    bidir = False,\n",
    "                    batch_size=256,\n",
    "                    sequence_length = 100)\n",
    "\n",
    "exp = Experiment(save_dir=os.getcwd())\n",
    "trainer = ptl.Trainer(experiment=exp, max_nb_epochs=20, train_percent_check=1, gpus=[0],track_grad_norm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T22:41:41.263229Z",
     "start_time": "2019-09-06T22:41:41.176518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Name              Type   Params\n",
      "0                 encoder           Encoder   683480\n",
      "1  encoder.word_embedding         Embedding   681000\n",
      "2            encoder.lstm              LSTM     2480\n",
      "3                 decoder           Decoder  1087200\n",
      "4            decoder.lstm              LSTM  1087200\n",
      "5         decoder.softmax           Softmax        0\n",
      "6                    loss  CrossEntropyLoss        0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (256, 300), got torch.Size([256, 100, 300])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-2b721ebf6dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# view tensorflow logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'and going to http://localhost:6006 on your browser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__single_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__single_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    788\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model, dataloader, max_batches, dataloader_i)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# RUN VALIDATION STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__validation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# track outputs for collation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__validation_forward\u001b[0;34m(self, model, data_batch, batch_i, dataloader_i)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# do non dp, ddp step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-f2226ae03b5b>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-f2226ae03b5b>\u001b[0m in \u001b[0;36mmy_loss\u001b[0;34m(self, y_hat, y)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 1834\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   1835\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (256, 300), got torch.Size([256, 100, 300])"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)\n",
    "\n",
    "# view tensorflow logs \n",
    "print(f'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}')\n",
    "print('and going to http://localhost:6006 on your browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Dimentionality Reduction with the Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Extract Features from encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:29:16.853325Z",
     "start_time": "2019-09-06T17:29:16.848545Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getEncoderFeatures(text):\n",
    "    model.eval()\n",
    "    out = model.encoder(torch.LongTensor([[message_field.vocab.stoi[word] for word in tokenizer(text)]]).to(DEVICE))\n",
    "    return out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:29:20.274689Z",
     "start_time": "2019-09-06T17:29:20.269589Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEmbeddingFeatures(text):\n",
    "    model.eval()\n",
    "    out = model.encoder.word_embedding(torch.LongTensor([[message_field.vocab.stoi[word] for word in tokenizer(text)]]).to(DEVICE))\n",
    "    return out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:44:28.350155Z",
     "start_time": "2019-09-06T17:44:18.993915Z"
    }
   },
   "outputs": [],
   "source": [
    "data2plot = pd.read_excel('dataset/data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Features from Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:46:18.069415Z",
     "start_time": "2019-09-06T17:44:42.668467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136915, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_AE = np.array([getEncoderFeatures(text) for text in data2plot.lemma.tolist()]).squeeze(1)\n",
    "features_AE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Features from Encoder Word Embedding to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:58:14.009470Z",
     "start_time": "2019-09-06T17:57:37.647045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136915, 300)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([getEmbeddingFeatures(text).squeeze(0).mean(0) for text in data2plot.lemma])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:59:19.985869Z",
     "start_time": "2019-09-06T17:58:28.696752Z"
    }
   },
   "outputs": [],
   "source": [
    "#BGM = BayesianGaussianMixture(n_components=20, covariance_type='full', max_iter=100).fit(features)\n",
    "kmeans = KMeans(n_clusters=20).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:59:20.185462Z",
     "start_time": "2019-09-06T17:59:19.987553Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = pd.DataFrame(kmeans.predict(features), columns = ['ClusterNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:59:49.403150Z",
     "start_time": "2019-09-06T17:59:46.864191Z"
    }
   },
   "outputs": [],
   "source": [
    "source, TOOLTIPS = preping_bokeh_clustering(data2plot,\n",
    "                                            features_AE,\n",
    "                                            'mensaje',\n",
    "                                            cluster)\n",
    "scatter(source,TOOLTIPS,len(cluster.ClusterNumber.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T21:27:37.965045Z",
     "start_time": "2019-09-04T21:26:36.098Z"
    }
   },
   "outputs": [],
   "source": [
    "# data2cluster = data2cluster.drop(columns = ['cluster'])\n",
    "data2cluster['cluster'] = cluster.ClusterNumber.tolist()\n",
    "\n",
    "data2cluster.to_excel('Conversaciones_Chat_Codensa_2019_onlyUser_cluster_fix3_kmeans.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
